# Copyright 2021 IRT Saint Exup√©ry, https://www.irt-saintexupery.com
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License version 3 as published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this program; if not, write to the Free Software Foundation,
# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
"""Test functions for the use of finite-differences approximation."""

from __future__ import annotations

import pytest
from gemseo.algos.design_space import DesignSpace
from gemseo.algos.doe.custom_doe.settings.custom_doe_settings import CustomDOE_Settings
from gemseo.algos.hashable_ndarray import HashableNdarray
from gemseo.algos.parameter_space import ParameterSpace
from gemseo.disciplines.analytic import AnalyticDiscipline
from gemseo.utils.derivatives.finite_differences import FirstOrderFD
from numpy import array
from numpy.testing import assert_allclose

from gemseo_umdo.formulations.sampling_settings import Sampling_Settings
from gemseo_umdo.formulations.taylor_polynomial_settings import (
    TaylorPolynomial_Settings,
)
from gemseo_umdo.scenarios.udoe_scenario import UDOEScenario


@pytest.mark.parametrize(
    ("statistic_estimation_settings", "expected"),
    [
        (
            Sampling_Settings(
                doe_algo_settings=CustomDOE_Settings(samples=array([[1.0]])),
                estimate_statistics_iteratively=False,
            ),
            [8.0, 16.0],
        ),
        (
            Sampling_Settings(
                doe_algo_settings=CustomDOE_Settings(samples=array([[1.0]])),
                estimate_statistics_iteratively=True,
            ),
            [8.0, 16.0],
        ),
        (TaylorPolynomial_Settings(second_order=False), [6.0, 12.0]),
        (TaylorPolynomial_Settings(second_order=True), [6.0, 12.0]),
    ],
)
def test_finite_differences(statistic_estimation_settings, expected):
    """Check that the finite-difference approximation of derivatives are ok."""
    discipline = AnalyticDiscipline({
        "f": "(x1+2*x2+u)**2",
        "c": "(x1+2*x2+u)**2",
        "o": "(x1+2*x2+u)**2",
    })

    design_space = DesignSpace()
    design_space.add_variable("x1", lower_bound=-1, upper_bound=1.0, value=0.5)
    design_space.add_variable("x2", lower_bound=-1, upper_bound=1.0, value=0.5)

    uncertain_space = ParameterSpace()
    uncertain_space.add_random_variable("u", "OTNormalDistribution")

    scenario = UDOEScenario(
        [discipline],
        "f",
        design_space,
        uncertain_space,
        "Mean",
        formulation_name="DisciplinaryOpt",
        statistic_estimation_settings=statistic_estimation_settings,
    )
    scenario.add_constraint("c", "Mean")
    scenario.add_observable("o", "Mean")
    scenario.set_differentiation_method("finite_differences")
    scenario.execute(algo_name="CustomDOE", samples=array([[1.0, 1.0]]), eval_jac=True)
    # The database storing the samples is cleared after each sampling.
    assert not scenario.mdo_formulation.optimization_problem.database

    # input_data_to_output_samples stores the samples
    # at points (x1, x2), (x1+dx1, x2) and (x1, x2+dx2)
    # where (x1, x2) is the design value at the current iteration.
    input_data_to_output_data = scenario.formulation.input_data_to_output_data
    assert len(input_data_to_output_data) == 3
    step = FirstOrderFD._DEFAULT_STEP
    for key, expected_key in zip(
        input_data_to_output_data,
        [
            HashableNdarray(array([1.0, 1.0])),
            HashableNdarray(array([1.0 - step, 1.0])),
            HashableNdarray(array([1.0, 1.0 - step])),
        ],
    ):
        assert key == expected_key

    get_history = (
        scenario.formulation.optimization_problem.database.get_gradient_history
    )

    grad_history = get_history("E[f]")
    assert_allclose(grad_history, array([[expected]]), atol=1e-3)

    grad_history = get_history("E[o]")
    assert_allclose(grad_history, array([[expected]]), atol=1e-3)

    grad_history = get_history("E[c]")
    assert_allclose(grad_history, array([[expected]]), atol=1e-3)
